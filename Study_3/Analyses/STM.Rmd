---
title: |2-
  The Effects of Metacognition in Survey Research -- Study 3 (Structural Topic Model)
author:
- affiliation: Ohio State University, School of Communication
  email: mattsweitzer@gmail.com
  name: Matthew D. Sweitzer
- affiliation: Ohio State University, School of Communication
  name: Hillary C. Shulman
date: "`r format(Sys.time(), '%B %e, %Y')`"
output:
  html_notebook:
    code_folding: none
#    self_contained: FALSE
#    theme: yeti
#    toc: yes
#    toc_depth: 3
---

# About this document

## Project description

This document is intended both as an open-source record of analyses conducted for one study in our paper entitled ["The Effects of Metacognition in Survey Research: Experimental, Cross-Sectional, and Content-Analytic Evidence"](https://github.com/Matt-Sweitzer/Papers/blob/master/SweitzerShulman2018.pdf), which was published recently in Public Opinion Quarterly, and as an introductory tutorial to Stuctural Topic Modeling (STM). If used for this latter purpose, this document is offered under the same license that governs the dataset used herein (available [here](https://github.com/Matt-Sweitzer/Metacognition_Surveys/blob/master/LICENSE)). Chiefly, this document is provided "AS IS", without warranty of any kind, express or implied. That said, if you encounter any issues, please feel free to email the first author using the link at the top of this document.

STMs are described in some detail in the paper. Additional resources are available [here](http://www.structuraltopicmodel.com/) and [here](http://scholar.harvard.edu/files/dtingley/files/topicmodelsopenendedexperiments.pdf). Briefly, STMs are a form of unsupervised machine learning used to categorize a corpus of text documents. What differentiates STMs from a "bag of words" or latent Dirichlet allocation approach is the ability to specify document metadata as covariates in the model.

We encourage you to read the paper linked above. A quick synopsis is required to understand the project we undertook. We are interested in whether the language complexity used in public opinion polling differentially affects metacognitive processes, and thus methodologically-pertinent outcomes such as "don't know" reporting, or abstaining from self-reporting. In study 1, we conducted a survey-experiment among a nationally-representative sample of U.S. adults. Participants were randomly assigned to either an "easy"- or "difficult"-language condition in which we varied the objective complexity of the language used to convey the public opinion questions. In study 2, we collected survey instruments from the Roper Center's database of public opinion polls (data and analyses coming soon; keep an eye on this website: [https://github.com/Matt-Sweitzer/](https://github.com/Matt-Sweitzer/)). Using top-line results, we compared question difficulty to rates of "don't know" reporting. Having found significant results for both studies, we turned our attention to the public opinion polling industry more broadly to see if language complexity varied systematically -- suggesting that the methodological problem posed by complex language may be widespread and unconsidered. One of the ways we looked for systematic variance in question complexity is by using a STM to categorize the questions into topics. Given that we had such a large dataset for this latter study, using a computational approach to code topics made this component of our analyses much more feasible.

## Session information from the most recent compiling

Before we begin, here is the most up-to-date information about the R session used to generate this file. To run this yourself, you will need to install the R packages `stm`, `stringr`, `tm`, `wordcloud`, and `SnowballC`. The latter two packages are only required to plot the wordcloud below and can be skipped for the topic model.

```{r echo=F, message=F}
library(SnowballC)
library(stm)
library(stringr)
library(tm)
library(wordcloud)
```

```{r echo=F}
format(Sys.time(), "%A %B %e, %Y - %I:%M:%S %p %Z")
sessionInfo()
```

# Analyses

## Read in and clean the data

The data for this study is available from my GitHub page.

```{r}
data<-read.csv(url("https://raw.githubusercontent.com/Matt-Sweitzer/Metacognition_Surveys/master/Study_3/Data/2016PollsFinal.csv"))
```

Let's take a look at the head of the data frame:

```{r echo=F, results='asis'}
library(knitr)
kable(head(data))
```

These variables can be interpreted as follows:

* `Date`: character, the date the results were published, or last date of data collection if publication date was not available
* `Poll`: character, the polling firm either responsible for data collection or the sponsor of third-party data collection
* `QuestionNum`: numeric, indicates the order of questions *within* the same ballot
* `QuestionWord`: character, wording of the question from the survey instrument
  + *Note*: we manually removed instructions to the surveyor (e.g., pronunciations, response ordering, etc.)
* `Region`: character, geographic region of survey sample
* `N`: numeric, total sample size
  + *Note*: some polling firms ask some questions of only some of their respondents (e.g., answered a prior question in a certain way). We opted to encode only the *total* sample size for every question on the same survey, as subsample sizes were not uniformly reported.
* `SurveyMethod`: character (factor), method of data collection -- options include "`online`", "`phone`", and "`phone/internet`"
* `Likely.vs.All`: character (factor), class of respondents targeted in sample -- options include "`All`", "`Likely`", and "`Registered`".
* `Ease`: numeric, Flesch reading ease score of `QuestionWord` -- calculated using the `koRpus` package in R
* `Grade`: numeric, Flesch-Kincaid grade level of `QuestionWord` -- calculated using the `koRpus` package in R
* `National`: numeric (factor), dummy variable collapsing `Region` -- 1 == "US", else 0
* `Geo`: character (factor), alternative collapsing of `Region` -- options include "`Community`", "`National`", and "`State`"
* `SurveyNum`: numeric (factor), indicates which survey instrument each question comes from -- should be identical to `Date`-`Poll` combination
* `Topic`: numeric, stm topic to which the question had the highest fit, $\theta$ -- this is what we will be estimating here!

Now, let's begin cleaning up the text corpus to make a basic corpus descriptive figure: a wordcloud. First, let's take the vector and change it to a corpus class:

```{r}
questionCorp<-Corpus(VectorSource(data$QuestionWord))
```

Next, lets start cleaning up some of the text. Wordclouds typically don't have puntucation listed. Also, we might expect that the most popular words in the English language, articles such as "the" or "a", would also be popular in our corpus -- showing them in this figure may not be very informative, so let's remove those too.

```{r warning=F}
questionCorp<-tm_map(questionCorp, removePunctuation)
questionCorp<-tm_map(questionCorp, removeWords, stopwords('english'))
```

Finally, some in the natural language processing realm advocate for a process called "stemming". This takes similar words and removes the suffixes which differentiate them so they can be treated as referring to the same thing. For example, "communicate", "communication", and "communicating" would all become "communicat". This can be somewhat helpful when those slight differentiations create excess noise in the data, or make for an especially sparse term-document matrix. Some of these stemming algorithms are not particularly robust -- your mileage may vary. I will not stem them here, but if you would like to stem your own corpus, uncomment the following line of code:

```{r}
#questionCorp<-tm_map(questionCorp, stemDocument)
```

Great! Now that the corpus is cleaned, we can take a look at the wordcloud:

```{r eval=F, fig.align='center', fig.width=8, fig.height=8}
  par(mar=c(0,0,0,0))
  wordcloud(questionCorp, max.words=1000, random.order=FALSE)
```
![](https://github.com/Matt-Sweitzer/Metacognition_Surveys/raw/master/Study_3/Analyses/wordcloud.png)

Clearly, the two lead candidate in the 2016 presidential election, Hillary Clinton and Donald Trump, were both asked about often in surveys leading up to that election. To plot this for yourself (and save the output), use the code below. Additional plotting commands are available, see `?wordcloud` for more information.

## *k*-values, search *k*, and diagnostic plots

## Running the STM

## Interpreting and exporting results